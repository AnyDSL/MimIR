target datalayout = "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64"
target triple = "nvptx64-unknown-cuda"

; The NVVM IR is equivalent to the following CUDA C code:
;
; __device__ int ave(int a, int b) {
;    return (a+b)/2;
; }
;
; __global__ void simple(int *data) {
;    int tid = blockIdx.x * blockDim.x + threadIdx.x;
;    data[tid] = ave(tid, tid);
; }
;
;
; texture<int, cudaTextureType1D, cudaReadModeElementType> tex;
; __global__ void simple_tex(int *data) {
;    int tid = blockIdx.x * blockDim.x + threadIdx.x;
;    data[tid] = tex1Dfetch(tex, tid);
; }
;
; __device__ __constant__ int cmem[32];
; __global__ void simple_cmem(int *data) {
;    int tid = blockIdx.x * blockDim.x + threadIdx.x;
;    data[tid] = cmem[threadIdx.x];
; }

define i32 @ave(i32 %a, i32 %b) {
entry:
  %add = add nsw i32 %a, %b
  %div = sdiv i32 %add, 2
  ret i32 %div
}

define void @simple(i32* %data) {
entry:
  %0 = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x()
  %1 = call i32 @llvm.nvvm.read.ptx.sreg.ntid.x()
  %mul = mul i32 %0, %1
  %2 = call i32 @llvm.nvvm.read.ptx.sreg.tid.x()
  %add = add i32 %mul, %2
  %call = call i32 @ave(i32 %add, i32 %add)
  %idxprom = sext i32 %add to i64
  %arrayidx = getelementptr inbounds i32* %data, i64 %idxprom
  store i32 %call, i32* %arrayidx, align 4
  ret void
}


@tex = addrspace(1) global i64 0, align 8

define void @simple_tex(i32* %data) {
entry:
  %0 = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x()
  %1 = call i32 @llvm.nvvm.read.ptx.sreg.ntid.x()
  %mul = mul i32 %0, %1
  %2 = call i32 @llvm.nvvm.read.ptx.sreg.tid.x()
  %add = add i32 %mul, %2

  %tex_ref = call i64 @llvm.nvvm.texsurf.handle.p1i64(metadata !{i64 addrspace(1)* @tex, metadata !"texture", i32 1}, i64 addrspace(1)* @tex)
  %tex_fetch = call { i32, i32, i32, i32 } asm sideeffect "tex.1d.v4.s32.s32 {$0,$1,$2,$3}, [$4, {$5,$6,$7,$8}];", "=r,=r,=r,=r,l,r,r,r,r" (i64 %tex_ref, i32 %add, i32 0, i32 0, i32 0)
  ; CUDA 7.0 supports texture reads via llvm.nvvm.tex.unified.1d
  ;%tex_fetch = call { i32, i32, i32, i32 } @llvm.nvvm.tex.unified.1d.v4s32.s32(i64 %tex_ref, i32 %add)
  %res = extractvalue { i32, i32, i32, i32 } %tex_fetch, 0

  ; llvm.nvvm.ldg not yet supported (CUDA 7.0)
  ;%res = tail call i32 @llvm.nvvm.ldg.global.i.i32.p0i32(i32 *%data, i32 4)

  %idxprom = sext i32 %add to i64
  %arrayidx = getelementptr inbounds i32* %data, i64 %idxprom
  store i32 %res, i32* %arrayidx, align 4
  ret void
}


@cmem = addrspace(4) global [32 x i32] zeroinitializer

define void @simple_cmem(i32* %data) {
entry:
  %0 = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x()
  %1 = call i32 @llvm.nvvm.read.ptx.sreg.ntid.x()
  %mul = mul i32 %0, %1
  %2 = call i32 @llvm.nvvm.read.ptx.sreg.tid.x()
  %add = add i32 %mul, %2
  %cidx = getelementptr inbounds [32 x i32] addrspace(4)* @cmem, i64 0, i32 %2
  %cval = load i32 addrspace(4)* %cidx, align 4
  %idxprom = sext i32 %add to i64
  %arrayidx = getelementptr inbounds i32* %data, i64 %idxprom
  store i32 %cval, i32* %arrayidx, align 4
  ret void
}


declare i32 @llvm.nvvm.read.ptx.sreg.ctaid.x() nounwind readnone

declare i32 @llvm.nvvm.read.ptx.sreg.ntid.x() nounwind readnone

declare i32 @llvm.nvvm.read.ptx.sreg.tid.x() nounwind readnone

declare i64 @llvm.nvvm.texsurf.handle.p1i64(metadata, i64 addrspace(1)*)

declare { i32, i32, i32, i32 } @llvm.nvvm.tex.unified.1d.v4s32.s32(i64, i32) nounwind readnone

;declare i32 @llvm.nvvm.ldg.global.i.i32.p0i32(i32 *, i32) nounwind readnone

!nvvm.annotations = !{!1, !2, !3, !4}
!nvvmir.version = !{!5}
!1 = metadata !{void (i32*)* @simple, metadata !"kernel", i32 1}
!2 = metadata !{void (i32*)* @simple_tex, metadata !"kernel", i32 1}
!3 = metadata !{i64 addrspace(1)* @tex, metadata !"texture", i32 1}
!4 = metadata !{void (i32*)* @simple_cmem, metadata !"kernel", i32 1}
!5 = metadata !{i32 1, i32 2}

