/// # The matrix Dialect {#mat}
///
/// [TOC]
///
/// ## Dependencies
///
.import mem;
.import core;
.import math;
// needed to access cps2ds
.import direct;
.import affine;
///
/// ## Types
///
/// ### %matrix.Mat
///
/// a n-dimensional tensor with elements of type T
/// can be seen as generalization of Coq's vector type
///
/// matrix = Π [n: .Nat, S: «n; .Nat», T: *] -> *
/// matrix n S T = «S_0; «S_1; ... «S_{n-1}; T» ... »»
/// => a matrix is a dependend array
///
.ax %matrix.Mat: Π [n: .Nat, S: «n; .Nat», T: *] -> *; 
///
/// ## Operations
///
/// ### %matrix.shape
/// 
/// gets the size along the i-th dimension
/// for a dependent matrix this is a simple projection
/// returns S(i)
///
/// normalization rules: 
/// * resolve shape calls at construction by replacing them with the size argument
.ax %matrix.shape:  Π [n: .Nat, S: «n; .Nat», T: *] -> [%matrix.Mat (n,S,T), i: .Idx n] -> .Nat, normalize_shape;
/// 
/// ### %matrix.const
///
/// a constant matrix
.ax %matrix.constMat: Π [n: .Nat, S: «n; .Nat», T: *] -> [%mem.M,T] -> [%mem.M,%matrix.Mat (n,S,T)];
/// 
/// ### %matrix.read
///
/// read _ (mat, idx) : body_type
///
/// a access to an element of the matrix
/// (currently: arithmetic pointer access)
/// normalization:
/// * read(insert)
/// * read(const)
.ax %matrix.read: Π [n: .Nat, S: «n; .Nat», T: *] -> [%mem.M, %matrix.Mat (n,S,T), idx: «i: n; .Idx S#i»] -> [%mem.M,T], normalize_read;
/// 
/// ### %matrix.insert
///
/// insert (dims, sizes, type) (mat, idx, val) : mat
///
/// depending on matrix implementation needs mem monad
/// as it is implemented as write
/// for mutable body types, the monad should be liftet
/// implementation either as write or array insertion
/// normalization:
/// * with other inserts
/// * with initialization
.ax %matrix.insert: Π [n: .Nat, S: «n; .Nat», T: *] -> [%mem.M,%matrix.Mat (n,S,T), idx: «i: n; .Idx S#i», val: T] -> [%mem.M,%matrix.Mat (n,S,T)], normalize_insert;
///
/// ### %matrix.init
///
/// a fresh matrix 
.ax %matrix.init: Π [n: .Nat, S: «n; .Nat», T: *, %mem.M] -> [%mem.M,%matrix.Mat (n,S,T)];
///
/// ### High-level matrix operations
///
// TODO: define alias: * fst, snd, split * zip = zipWith id
.ax %matrix.prod:  Π [m: .Nat, k: .Nat, l: .Nat, [p: .Nat, e:.Nat]] -> 
    [%mem.M,%matrix.Mat (2,(m, k),%math.F (p,e)), %matrix.Mat (2,(k, l),%math.F (p,e))] -> [%mem.M,%matrix.Mat (2,(m, l),%math.F (p,e))], normalize_prod;
.ax %matrix.transpose: Π [[k:.Nat, l:.Nat], T: *] -> 
    [%mem.M,%matrix.Mat (2,(k,l),T)] -> [%mem.M,%matrix.Mat (2,(l,k),T)], normalize_transpose;
.ax %matrix.sum: Π [n: .Nat, S: «n; .Nat», [p:.Nat,e:.Nat]] -> [%mem.M,%matrix.Mat (n,S,%math.F (p,e))] -> [%mem.M,%math.F (p,e)];
///
// TODO: handle reduction case: n=0, S=[] => not empty but scalar
/// Our notation is inspired by einsum (with some generalizations):
/// * Tensorflow / XLA: einsum
/// * Pytorch: einsum
/// * NumPy: einsum
/// * Halide
/// * Haskell: Tensor DSL
/// * Ricci Calculus
/// * Einstein Notation
/// * Pytorch DSL
/// * https://optimized-einsum.readthedocs.io/en/stable/
///
/// The `mapReduce` operation can be seen as the minimal abstraction over general iteration/control flow schemes over tensors.
///
/// mapReduce applications:
/// * `einsum(idx, MatrixIndices) = mapReduce(0,+,product,MatrixIndices)`
/// * `map f M = mapReduce (0,+,f,[(idx,M)])` (TODO: get rid of reduce step if not needed with dummy values)
/// * `reduce acc f M = mapReduce (n=0) (acc,f,id,[(idx,M)])` (TODO: see index problem above)
/// einsum application:
/// * `tranpose ij->ji  (einsum(,[(1,0),M]))`
/// * `trace ii->`
/// * `sum ij ->`
/// * `col sum ij -> j `
/// * `mat vec prod ik,k->i`
/// * `mat mat prod ik,kj -> ij`
/// * `dot product i,i ->`
/// * `dot matrix ij,ij ->`
/// * `outer product i,j -> ij`
/// TODO: introduce dummy values (zero, add, ...) in refly and use these
/// dummy = has correct type but can not produce code (should always be eliminated)
.ax %matrix.mapReduce: 
    // out shape depends on in shape but is complex
    Π [n: .Nat, S: «n; .Nat», T: *, // out shape
        m: .Nat, // number of inputs
        NI: «m; .Nat», // input dimensions
        TI: «m; *», // input types
        SI: «i:m; «NI#i; .Nat»» // input shapes
    ] -> 
    // main arguments
    [
        mem: %mem.M, // memory
        zero: T, // initial value
        // TODO: propagate change: no addition but instead take acc as argument (like mlir.linarith.generic)
        comb: .Cn[[%mem.M, T, «i: m; TI#i»],.Cn[%mem.M,T]], // inner combination
        // out_index not needed => always ij (0 ... n) for n dimensions
        input:
            «i:m; 
                [
                    «NI#i;.Nat»,
                    %matrix.Mat (NI#i,SI#i,TI#i)
                ]
            »
    ] ->
    [%mem.M, %matrix.Mat (n,S,T)],
    normalize_mapReduce;
///
///
/// ## Unfolding functions
/// 
/// ### product
/// 
/// Follow the principle `ij <- ik,kj` (`out[i,j] = sum_k in1[i,k] * in2[k,j]`) by using mulplication as combination function and addition as reduction function.
.lam .extern internal_mapRed_matrix_prod 
    ![m: .Nat, k: .Nat, l: .Nat, [p: .Nat, e:.Nat]] ->
    (.Cn[
        [%mem.M,%matrix.Mat (2,(m, k),%math.F (p,e)), %matrix.Mat (2,(k, l),%math.F (p,e))],
        .Cn[%mem.M,%matrix.Mat (2,(m, l),%math.F (p,e))]
    ]) 
    = {
    .let R = %math.F (p,e);

    .con prod_comb [[mem:%mem.M, acc:R, [a:R, b:R]], ret:.Cn[%mem.M,R]] = {
        .let v = %math.arith.mul 0 (a,b);

        // reduce op = addition
        .let new_acc = %math.arith.add 0 (acc,v);
        ret (mem, new_acc)
    };
    .con inner_matrix_prod 
        ![
            [
                mem:%mem.M,
                M:%matrix.Mat (2,(m, k),R), 
                N: %matrix.Mat (2,(k, l),R)
            ],
            ret: .Cn[%mem.M,%matrix.Mat (2,(m, l),R)]
        ]
    = {
        .let zero_64 = 0.0:(%math.F (52,11));
        .let zero_real = %math.conv.f2f (p,e) zero_64;
        ret (
            %matrix.mapReduce
                (2, (m, l), R, 
                    2,
                    (2, 2),
                    (R,R),
                    ((m,k),(k,l))
                )
                (
                    mem,
                    zero_real,
                    prod_comb,
                    (
                        ((0,2), M),
                        ((2,1), N)
                    )
                )
        )
    };
    inner_matrix_prod
};
/// 
/// ### transpose
/// 
/// Transpose a matrix by iterating the indices in swapped order.
// TODO: check code for 1-matrix edge case
// TODO: would this automatically be handled by read(transpose) ?
.lam .extern internal_mapRed_matrix_transpose
    ![[k: .Nat, l: .Nat], T:*] ->
    (.Cn[
        [%mem.M,%matrix.Mat (2,(k, l),T)],
        .Cn[%mem.M,%matrix.Mat (2,(l, k),T)]
    ]) 
    = {
    .con transpose_comb [[mem:%mem.M, acc:T, [a:T]], ret:.Cn[%mem.M,T]] = {
        // We ignore the (zero) accumulator and just return the read value.
        .let new_acc = a;
        ret (mem, new_acc)
    };
    .con inner_matrix_transpose
        ![
            [
                mem:%mem.M,
                M:%matrix.Mat (2,(k, l),T), 
            ],
            ret: .Cn[%mem.M,%matrix.Mat (2,(l, k),T)]
        ]
    = {
        // TODO: use generalized zero
        .let zero = (⊥:T);
        ret (
            %matrix.mapReduce
                (2, (l, k), T, 
                    1,
                    2,
                    T,
                    (k,l)
                )
                (
                    mem,
                    zero,
                    transpose_comb,
                    (
                        ((1,0), M)
                    )
                )
        )
    };
    inner_matrix_transpose
};
/// 
/// ### sum
/// 
/// Sums up all elements of a matrix and returns a scalar.
//  TODO: test 0d matrix (edge cases in code)
.lam .extern internal_mapRed_matrix_sum
    ![n: .Nat, S: «n; .Nat», [p:.Nat,e:.Nat]] ->
    (.Cn[
        [%mem.M,%matrix.Mat (n,S,%math.F (p,e))],
        .Cn[%mem.M,%math.F (p,e)]
    ]) 
    = {
    .let R = %math.F (p,e);
    .con sum_comb [[mem:%mem.M, acc:R, [a:R]], ret:.Cn[%mem.M,R]] = {
        .let new_acc = %math.arith.add 0 (acc,a);
        ret (mem, new_acc)
    };
    .con inner_matrix_sum
        ![
            [
                mem:%mem.M,
                M:%matrix.Mat (n,S,R), 
            ],
            ret: .Cn[%mem.M,R]
        ]
    = {
        // TODO: use generalized zero
        .let zero_64 = 0.0:(%math.F (52,11));
        .let zero_real = %math.conv.f2f (p,e) zero_64;
        // should be normalized to lit tuple
        .let idxs =
            <i:n;
            %core.nop.add (1,
                %core.bitcast (.Nat, .Idx n) i
            )
            >;
        .let (mem2,res) = %matrix.mapReduce
                (1, (1), R, 
                    1,
                    n,
                    R,
                    S
                )
                (
                    mem,
                    zero_real,
                    sum_comb,
                    (
                        (idxs, M)
                    )
                );
        ret (mem2,
            // TODO: test this cast
            %core.bitcast (
                R,
                %matrix.Mat (1,1,R)
            ) res
        )
    };
    inner_matrix_sum
};



///
/// ## Compilation Passes and Phases
/// 
/// ### Passes
/// 
.ax %matrix.lower_matrix_high_level_map_reduce: %compile.Pass;
.ax %matrix.lower_matrix_medium_level: %compile.Pass;
.ax %matrix.lower_matrix_low_level: %compile.Phase;
.ax %matrix.internal_map_reduce_cleanup: %compile.Pass;
/// 
/// ### Phases
/// 
.let matrix_lower_phase = {
  %compile.phases_to_phase (⊤:.Nat)
    (
        (%compile.pass_phase (%compile.pass_list
            %matrix.lower_matrix_high_level_map_reduce
            %matrix.lower_matrix_medium_level
        )),
        // TODO: only in map_red namespace
        %compile.single_pass_phase %matrix.internal_map_reduce_cleanup,
        %matrix.lower_matrix_low_level
    )
};
